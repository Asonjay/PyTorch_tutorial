{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE6521(AU22): PyTorch Tutorial**\n",
    "\n",
    "### Author: [Zexin (Jason) Xu](https://asonjay.github.io)\n",
    "\n",
    "This notebook will serve as a basic introduction to PyTorch. This notebook will include 3 sessions: tensor, neural network, and a sample NLP task. After finishing this notebook, you will be able to build a fully-connected feed-forward neural network with PyTorch.\n",
    "\n",
    "### Credit\n",
    "* [\"Word Window Classification\" tutorial notebook](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/materials/ww_classifier.ipynb) by Matt Lamm, from Winter 2020 offering of CS224N\n",
    "* CSE224N: PyTorch Tutorial (Winter '22) by Dilara Soylu, Ethan Chi\n",
    "* Official PyTorch Documentation on [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by Soumith Chintala\n",
    "* PyTorch Tutorial Notebook, [Build Basic Generative Adversarial Networks (GANs) | Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans) by Sharon Zhou, offered on Coursera\n",
    "\n",
    " Thanks ``Dr. Yu Su`` for his precious feedback!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "0. Packages\n",
    "1. Tensors\n",
    "    - Create a tensor\n",
    "    - Data Type\n",
    "    - Conversion with NumPy array\n",
    "    - Initialize a tensor\n",
    "    - Tensor view/reshape\n",
    "    - Matrix operations\n",
    "    - Vectorized operations\n",
    "    - Indexing\n",
    "2. Neural Network\n",
    "    - Layers\n",
    "        - Linear layer\n",
    "        - Activation layer\n",
    "    - Pile them up!\n",
    "    - Create your own nn.Modules!\n",
    "    - Basics: backward() and \"grad\"\n",
    "    - Optimization\n",
    "    - loss functions\n",
    "3. Demo - Heart Disease Indicators\n",
    "    - Collecting data\n",
    "    - Train/valid/test split\n",
    "    - Batch/dataloader\n",
    "    - Feed-forward neural network\n",
    "    - Training\n",
    "    - Evaluation & confusion matrix\n",
    "    - What is wrong?\n",
    "4. What is next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Packages \n",
    "\n",
    "[PyTorch](https://pytorch.org/) is an open source machine learning framework. Other popular framework on the market is [TensorFlow](https://www.tensorflow.org/). Feel free to chek it out.\n",
    "\n",
    "First off, let's import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Tensors\n",
    "In this session, we will learn the basics about tensors, along with some basic manipulations about it. In short, tensor is a multi-dimensional matrix. And it is the most basic and important component of PyTorch framework. \n",
    "\n",
    "Without future ado, let's learn about tensors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tensor\n",
    "Tensor can be created with **array** during initialization. Here, we are creating a `3x2` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[0, 1], \n",
    "                       [2, 3], \n",
    "                       [4, 5]])\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type\n",
    "Each Tensor can have [**data types**](https://pytorch.org/docs/stable/tensors.html). You can specify the `dtype` attribute while creating the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]], dtype=torch.int32)\n",
      "-------------------------\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n",
      "-------------------------\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with np.array\n",
    "np_arr = np.array([[0, 1],\n",
    "                   [2, 3],\n",
    "                   [4, 5]])\n",
    "tensor = torch.tensor(np_arr)\n",
    "print(tensor)\n",
    "print('-' * 25)\n",
    "\n",
    "# Create a tensor with given data type\n",
    "tensor = torch.tensor([[0, 1],\n",
    "                       [2, 3],\n",
    "                       [4, 5]], dtype=torch.float32)\n",
    "print(tensor)\n",
    "print('-' * 25)\n",
    "\n",
    "# Create a tensor with long type (Tensor)\n",
    "tensor = torch.LongTensor(np_arr)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion with NumPy array\n",
    "Note that when creating a tensor, you can also convert a **NumPy array** to a tensor. And of course, you can do it other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]], dtype=torch.int32)\n",
      "-------------------------\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with from_numpy function\n",
    "tensor = torch.from_numpy(np.array([[0, 1], \n",
    "                                    [2, 3], \n",
    "                                    [4, 5]]))\n",
    "print(tensor)\n",
    "print('-' * 25)\n",
    "\n",
    "# Convert a tensor to numpy array\n",
    "np_arr = tensor.numpy()\n",
    "print(np_arr)\n",
    "print(type(np_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialze a tensor\n",
    "There are more ways to initialize a tensor. Below are multiple ways of doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "-------------------------\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "-------------------------\n",
      "tensor([[0.8569, 0.3636],\n",
      "        [0.2975, 0.5786],\n",
      "        [0.0524, 0.0527]])\n",
      "tensor([[ 2.1447,  4.5004],\n",
      "        [11.8152,  6.1781],\n",
      "        [ 9.2662,  8.8248]])\n",
      "-------------------------\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize a tensor with zeros\n",
    "zeros = torch.zeros(3, 2)\n",
    "print(zeros) \n",
    "print('-' * 25)\n",
    "\n",
    "# Initialize a tensor with ones\n",
    "ones = torch.ones(3, 2) \n",
    "print(ones)\n",
    "print('-' * 25)\n",
    "\n",
    "# Initialize a tensor with random values\n",
    "# By default, torch.rand() returns a tensor with floating points values in [0, 1)\n",
    "randoms = torch.rand(3, 2)  \n",
    "print(randoms)\n",
    "# Initialize a tensor with random values in [2, 12)\n",
    "randoms1 = torch.rand(3, 2) * 10 + 2\n",
    "print(randoms1)\n",
    "print('-' * 25)\n",
    "\n",
    "# Initialize a tensor with empty value\n",
    "# Even though you can still print the empty tensor, but note that torch.empty() returns # a tensor filled with uninitialized data\n",
    "empties = torch.empty(3, 2)\n",
    "print(empties) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor view/reshape\n",
    "Since tensor is a multi-dimensional matrix, PyTorch provides couple utilities to view/manipulate its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "-------------------------\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of a tensor\n",
    "tensor = torch.tensor([[0, 1], \n",
    "                       [2, 3], \n",
    "                       [4, 5]])\n",
    "print(tensor.shape)\n",
    "print('-' * 25)\n",
    "\n",
    "# Reshape a tensor\n",
    "tensor = torch.tensor([[0, 1], \n",
    "                       [2, 3], \n",
    "                       [4, 5]])\n",
    "tensor_reshape = tensor.reshape(2, 3)\n",
    "print(tensor_reshape)\n",
    "print(tensor_reshape.shape)\n",
    "print(tensor.shape)\n",
    "\n",
    "# --- Difference between view and reshape ---\n",
    "# view() will try to change the shape of the tensor while keeping the underlying data \n",
    "# allocation the same, thus data will be shared between the two tensors. reshape() will # create a new underlying memory allocation if necessary. Thus, view() will require \n",
    "# your operation to be contiguous, while reshape() will not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix operations\n",
    "Tensor can perform matrix-like operations as well, such as element-wise addition/multiplication, and matrix multiplication, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  8],\n",
      "        [10, 12],\n",
      "        [14, 16]])\n",
      "-------------------------\n",
      "tensor([[ 0,  7],\n",
      "        [16, 27],\n",
      "        [40, 55]])\n",
      "-------------------------\n",
      "tensor([[ 3,  4,  5],\n",
      "        [ 9, 14, 19],\n",
      "        [15, 24, 33]])\n",
      "tensor([[ 3,  4,  5],\n",
      "        [ 9, 14, 19],\n",
      "        [15, 24, 33]])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise addition/multiplication\n",
    "a = torch.tensor([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5]])\n",
    "b = torch.tensor([[6, 7],\n",
    "                  [8, 9],\n",
    "                  [10, 11]])\n",
    "print(a + b)\n",
    "print('-' * 25)\n",
    "print(a * b)\n",
    "print('-' * 25)\n",
    "\n",
    "# Matrix multiplication\n",
    "c = torch.tensor([[0, 1, 2],\n",
    "                  [3, 4, 5]])\n",
    "print(a.matmul(c))\n",
    "print(a @ c) # Other way to do matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized operations\n",
    "Like matrix, tensor can perform **vectorized operations**: operations that be conducted in parallel over a particular dimension of a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "-------------------------\n",
      "tensor([12., 15., 18., 21.])\n",
      "tensor([ 6., 22., 38.])\n",
      "-------------------------\n",
      "tensor([[[ 0.,  1.],\n",
      "         [ 2.,  3.]],\n",
      "\n",
      "        [[ 4.,  5.],\n",
      "         [ 6.,  7.]],\n",
      "\n",
      "        [[ 8.,  9.],\n",
      "         [10., 11.]]])\n",
      "-------------------------\n",
      "tensor([[12., 15.],\n",
      "        [18., 21.]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [10., 12.],\n",
      "        [18., 20.]])\n",
      "tensor([[ 1.,  5.],\n",
      "        [ 9., 13.],\n",
      "        [17., 21.]])\n",
      "-------------------------\n",
      "tensor([[4., 5.],\n",
      "        [6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.arange(0, 12, dtype=torch.float32).reshape(3, 4)\n",
    "print(tensor1)\n",
    "print('-' * 25)\n",
    "# sum over different dimensions\n",
    "print(tensor1.sum(dim=0))\n",
    "print(tensor1.sum(dim=1))\n",
    "print('-' * 25)\n",
    "\n",
    "tensor2 = torch.arange(0, 12, dtype=torch.float32).reshape(3, 2, 2)\n",
    "print(tensor2)\n",
    "print('-' * 25)\n",
    "# sum over different dimensions\n",
    "print(tensor2.sum(dim=0))\n",
    "print(tensor2.sum(dim=1))\n",
    "print(tensor2.sum(dim=2))\n",
    "print('-' * 25)\n",
    "\n",
    "# Other operations (std, mean, max, min, argmax, argmin, etc.)\n",
    "print(tensor2.mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "Tensor elements can be accessed with `[]` like array. In the meanwhile, you can also pass a tensor in `[]`. Note that that data type of index tensor should be a int/long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.],\n",
      "         [ 2.,  3.],\n",
      "         [ 4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.],\n",
      "         [ 8.,  9.],\n",
      "         [10., 11.]],\n",
      "\n",
      "        [[12., 13.],\n",
      "         [14., 15.],\n",
      "         [16., 17.]],\n",
      "\n",
      "        [[18., 19.],\n",
      "         [20., 21.],\n",
      "         [22., 23.]]])\n",
      "-------------------------\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n",
      "tensor([2., 3.])\n",
      "tensor(3.)\n",
      "-------------------------\n",
      "tensor([[ 8.,  9.],\n",
      "        [14., 15.]])\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0, 24, dtype=torch.float32).reshape(4, 3, 2)\n",
    "print(tensor)\n",
    "print('-' * 25)\n",
    "\n",
    "print(tensor[0])\n",
    "print(tensor[0, 1])\n",
    "print(tensor[0, 1, 1])\n",
    "print('-' * 25)\n",
    "\n",
    "print(tensor[[1, 2], 1]) # Same as tensor[[1:3, 1]]\n",
    "print('-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.],\n",
      "         [ 2.,  3.]],\n",
      "\n",
      "        [[ 4.,  5.],\n",
      "         [ 6.,  7.]],\n",
      "\n",
      "        [[ 8.,  9.],\n",
      "         [10., 11.]]])\n",
      "-------------------------\n",
      "tensor([[[0., 1.],\n",
      "         [2., 3.]],\n",
      "\n",
      "        [[4., 5.],\n",
      "         [6., 7.]],\n",
      "\n",
      "        [[0., 1.],\n",
      "         [2., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing with a tensor\n",
    "tensor = torch.arange(0, 12, dtype=torch.float32).reshape(3, 2, 2)\n",
    "print(tensor)\n",
    "print('-' * 25)\n",
    "index = torch.tensor([0, 1, 0]) # Reprint 0th row two times\n",
    "print(tensor[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Neural Network (nn.module)\n",
    "\n",
    "In this session, we will look at some basic components to build a simple neural network from scractch using PyTorch. \n",
    "\n",
    "Layers are important during forward propagation. In this tutorial I will introduce some basic layers (nn.linear() and some activation functions). We are going to building our layers with `nn.sequential()`.\n",
    "\n",
    "In the meanwhile, calculating gradients and back propagates the loss are significant when training our model. `backwards()`, **gradients, optimizer, and loss functions**  are the components in PyTorch to realize this.\n",
    "\n",
    "After this session, you will develop a basic understanding of components that PyTorch offers to help us build our own neural network. Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "PyTorch provides a tons of [layers](https://pytorch.org/docs/stable/nn.html#). In this tutorial, we will mainly talk about building a neural network with linear layers.\n",
    "\n",
    "### Linear Layer\n",
    "Applies a linear transformation to the incoming data: \n",
    "y = xA^T + b (x = Input, y = Output, A = weight, b = bias)\n",
    "\n",
    "Input: (∗,H_in) where ∗ means any number of dimensions including none and H_in = in_features.\n",
    "\n",
    "Output: (∗,H_out) where all but the last dimension are the same shape as the input and H_out = out_features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "-------------------------\n",
      "tensor([[[ 0.7863,  0.3632, -0.1071],\n",
      "         [ 0.7863,  0.3632, -0.1071],\n",
      "         [ 0.7863,  0.3632, -0.1071]],\n",
      "\n",
      "        [[ 0.7863,  0.3632, -0.1071],\n",
      "         [ 0.7863,  0.3632, -0.1071],\n",
      "         [ 0.7863,  0.3632, -0.1071]]], grad_fn=<ViewBackward0>)\n",
      "-------------------------\n",
      "[('weight', Parameter containing:\n",
      "tensor([[ 0.1067,  0.0534,  0.4368, -0.0046],\n",
      "        [ 0.1344, -0.1546,  0.3087, -0.3330],\n",
      "        [ 0.4091, -0.2223, -0.4721, -0.2260]], requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([0.1940, 0.4076, 0.4042], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "inp = torch.ones(2, 3, 4)\n",
    "print(inp)\n",
    "print('-' * 25)\n",
    "\n",
    "linear = nn.Linear(4, 3)\n",
    "print(linear(inp))\n",
    "print('-' * 25)\n",
    "# Check closely, we are transforming the input from 2x3x4 to 2x3x3\n",
    "# nn.Linear applies a linear transformation to the last dimension of the input\n",
    "\n",
    "print(list(linear.named_parameters())) # The weights and bias are randomly initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Layer\n",
    "Activation functions are used to add non-linearity to our network. Activation functions will not change the dimension of the input, thus it somewhat \"activates\" the input value by scaling it up or down. We are going to try out some basic functions as shown below:\n",
    "1. Sigmoid\n",
    "2. ReLU\n",
    "3. Tanh\n",
    "4. Softmax / LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3810, 0.0616, 0.8571],\n",
      "        [0.2484, 0.9643, 0.5612]])\n",
      "-------------------------\n",
      "tensor([[0.5941, 0.5154, 0.7020],\n",
      "        [0.5618, 0.7240, 0.6367]])\n",
      "-------------------------\n",
      "tensor([[0.3810, 0.0616, 0.8571],\n",
      "        [0.2484, 0.9643, 0.5612]])\n",
      "-------------------------\n",
      "tensor([[0.3636, 0.0615, 0.6947],\n",
      "        [0.2434, 0.7462, 0.5089]])\n",
      "-------------------------\n",
      "tensor([[0.5331, 0.2885, 0.5734],\n",
      "        [0.4669, 0.7115, 0.4266]])\n"
     ]
    }
   ],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "ReLU = nn.ReLU()\n",
    "tanh = nn.Tanh()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "\n",
    "tensor = torch.rand(2, 3)\n",
    "print(tensor)\n",
    "print('-' * 25)\n",
    "\n",
    "# Apply different activation functions to same tensor\n",
    "# Check how the values are transformed with different activation functions\n",
    "out = sigmoid(tensor)\n",
    "print(out)\n",
    "print('-' * 25)\n",
    "out = ReLU(tensor)\n",
    "print(out)\n",
    "print('-' * 25)\n",
    "out = tanh(tensor)\n",
    "print(out)\n",
    "print('-' * 25)\n",
    "out = softmax(tensor)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pile them up!\n",
    "To build a neural network, we will need multiple layers. And here is how we do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4980, 0.4933],\n",
      "         [0.5055, 0.4937],\n",
      "         [0.5017, 0.4970],\n",
      "         [0.5021, 0.4943]],\n",
      "\n",
      "        [[0.5020, 0.5067],\n",
      "         [0.4945, 0.5063],\n",
      "         [0.4983, 0.5030],\n",
      "         [0.4979, 0.5057]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.rand(2, 4, 6)\n",
    "\n",
    "fc1 = nn.Linear(6, 4) \n",
    "ac1 = nn.Sigmoid()\n",
    "fc2 = nn.Linear(4, 2)\n",
    "ac2 = nn.ReLU()\n",
    "fc3 = nn.Linear(2, 2)\n",
    "softmax = nn.Softmax(dim=0)\n",
    "\n",
    "out = fc1(inp)\n",
    "out = ac1(out)\n",
    "out = fc2(out)\n",
    "out = ac2(out)\n",
    "out = fc3(out)\n",
    "out = softmax(out)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is so redundant. Luckily, PyTorch provides us with a more compact module to pile our layers up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2472, 0.2518],\n",
      "         [0.2509, 0.2516],\n",
      "         [0.2502, 0.2478],\n",
      "         [0.2517, 0.2488]],\n",
      "\n",
      "        [[0.2495, 0.2488],\n",
      "         [0.2503, 0.2486],\n",
      "         [0.2463, 0.2528],\n",
      "         [0.2539, 0.2498]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.rand(2, 4, 6)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(4, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "print(model(inp)) # nice and clean :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your own nn.Modules!\n",
    "So far you have equipped with basic knowledge to build your own neural network class! Here is what basic structure looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5382, 0.4697],\n",
      "         [0.5660, 0.5021],\n",
      "         [0.5391, 0.4662],\n",
      "         [0.4726, 0.4748]],\n",
      "\n",
      "        [[0.5094, 0.4699],\n",
      "         [0.5244, 0.4723],\n",
      "         [0.4983, 0.4488],\n",
      "         [0.5111, 0.4967]]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FFNN(nn.Module):\n",
    "\n",
    "  def __init__(self, inp, hid, out):\n",
    "    # Call to the __init__ function of the super class\n",
    "    super(FFNN, self).__init__()\n",
    "    # Init parameters \n",
    "    self.input_size = inp\n",
    "    self.hidden_size = hid\n",
    "    self.output_size = out\n",
    "    # Building models\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(self.input_size, self.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.hidden_size, self.output_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x): \n",
    "    return self.model(x)\n",
    "  \n",
    "# Let's test it with some random input\n",
    "input = torch.rand(2, 4, 6)\n",
    "\n",
    "ffnn = FFNN(inp=6, \n",
    "            hid=4, \n",
    "            out=2)\n",
    "\n",
    "print(ffnn(input)) # same as \"print(ffnn.forward(input))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('model.0.weight', Parameter containing:\n",
      "tensor([[ 0.0333,  0.0981, -0.0923, -0.3743,  0.0235,  0.3826],\n",
      "        [ 0.0529,  0.0706, -0.1433,  0.1922, -0.0642,  0.3250],\n",
      "        [-0.2854,  0.3561,  0.0471,  0.3494, -0.0483,  0.0387],\n",
      "        [ 0.3905,  0.0492,  0.2485,  0.0249,  0.2655,  0.2105]],\n",
      "       requires_grad=True)), ('model.0.bias', Parameter containing:\n",
      "tensor([-1.1215e-01,  3.9474e-01,  2.4125e-04,  1.9658e-01],\n",
      "       requires_grad=True)), ('model.2.weight', Parameter containing:\n",
      "tensor([[ 0.2055,  0.1479,  0.4395, -0.3037],\n",
      "        [ 0.3720, -0.3727,  0.0471, -0.2854]], requires_grad=True)), ('model.2.bias', Parameter containing:\n",
      "tensor([0.1303, 0.3216], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(ffnn.named_parameters())) # Check parameters\n",
    "# One thing to note that, only linear layers have parameters\n",
    "# while activation functions do not have any parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics: backward() and \"grad\"\n",
    "Now, we have our model. But that is not enough. Before taking about how to calculate the loss generated, let's first talk about how we back propagates the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], requires_grad=True)\n",
      "tensor([1.])\n",
      "-------------------------\n",
      "tensor([61.])\n",
      "tensor([121.])\n",
      "tensor([181.])\n",
      "tensor([241.])\n",
      "tensor([301.])\n",
      "tensor([361.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5], dtype=torch.float32, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "x.backward()\n",
    "print(x.grad) # Gradient of x with respect to itself / d(x)/d(x) = 1\n",
    "print('-' * 25)\n",
    "\n",
    "y = 6 * x ** 2\n",
    "y.backward() # d(y)/d(x) + d(x)/d(x) = d(6x^2)/d(x) + 1 = 12x + 1 = 12 * 5 + 1 = 61\n",
    "print(x.grad)\n",
    "\n",
    "# ===================\n",
    "# So far x.grad is updated accumulatively. This is also the nature when we do back \n",
    "# propagation - we are summing up all the losses. However, one thing to keep in mind\n",
    "# that we need to reset the gradients to zero using `zero_grad()` after one iteration\n",
    "# so that the losses are calculated correctly.\n",
    "# ===================\n",
    "\n",
    "# This shows how the gradients are accumulated\n",
    "for i in range(5):\n",
    "    y = 6 * x ** 2\n",
    "    y.backward()\n",
    "    print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Now, let's use a toy task to demostrate how to calculate loss, propagates it and updates the parameters. \n",
    "\n",
    "`torch.optim` modules contains multiple [optimizers](https://pytorch.org/docs/stable/optim.html) we can use. Then, `.parameters()` will be able to tell optimizer what parameters to update. `backward()` will then back propagates the gradient calculated.\n",
    "\n",
    "**Toy Task:** \n",
    "\n",
    "getting a random tensor [0, 1), ground truth is a tensor with all 1s. After optimizing and back propagates, the model should train the parameters and output should be close to all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9570, 0.3133],\n",
      "        [0.8001, 0.9954]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "Loss = 0.7061628103256226\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "\n",
    "  def __init__(self, inp, hid, out):\n",
    "    # Call to the __init__ function of the super class\n",
    "    super(FFNN, self).__init__()\n",
    "    # Init parameters \n",
    "    self.input_size = inp\n",
    "    self.hidden_size = hid\n",
    "    self.output_size = out\n",
    "    # Building models\n",
    "    self.model = nn.Sequential(\n",
    "        nn.Linear(self.input_size, self.hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.hidden_size, self.output_size),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x): \n",
    "    return self.model(x)\n",
    "\n",
    "input = torch.rand(2, 2)\n",
    "print(input)\n",
    "goal = torch.ones(2, 2)\n",
    "print(goal)\n",
    "\n",
    "ffnn = FFNN(inp=2,\n",
    "            hid=4,\n",
    "            out=2)\n",
    "\n",
    "## nn's parameters are passed into Adam with .parameters()\n",
    "adam = optim.Adam(ffnn.parameters(), lr=0.05)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "pred = ffnn(input) # same as nn.forward(input)\n",
    "print(f'Loss = {loss_function(pred, goal).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the loss function, what we need to do is just to updates our loss and updates the parameter. Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial input:\n",
      "tensor([[0.9570, 0.3133],\n",
      "        [0.8001, 0.9954]])\n",
      "-------------------------\n",
      "Epoch 0/100 - Loss = 0.7061628103256226\n",
      "Epoch 10/100 - Loss = 0.2066076397895813\n",
      "Epoch 20/100 - Loss = 0.0243238378316164\n",
      "Epoch 30/100 - Loss = 0.004261927213519812\n",
      "Epoch 40/100 - Loss = 0.0016252856003120542\n",
      "Epoch 50/100 - Loss = 0.0010026042582467198\n",
      "Epoch 60/100 - Loss = 0.0007797690341249108\n",
      "Epoch 70/100 - Loss = 0.0006714300834573805\n",
      "Epoch 80/100 - Loss = 0.0006035299156792462\n",
      "Epoch 90/100 - Loss = 0.000552400597371161\n",
      "-------------------------\n",
      "Final prediction:\n",
      "tensor([[0.9992, 0.9989],\n",
      "        [0.9999, 0.9999]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('Initial input:')\n",
    "print(input)\n",
    "print('-' * 25)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    adam.zero_grad()\n",
    "    pred = ffnn(input)\n",
    "    loss = loss_function(pred, goal)\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs} - Loss = {loss.item()}')\n",
    "print('-' * 25)\n",
    "print('Final prediction:')\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad huh? Our model successfully predicts our input as close to torch.ones(2, 2) as possible! Good job :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Demo - Heart Disease Indicators\n",
    "We have learned all the basics of PyTorch and we are ready to build a network to solve a problem! In this session, I will use a simple dataset to demonstrate how to build a PyTorch neural network from scratch.\n",
    "\n",
    "This model will use a data science approach to indicate **whether a patient will have a heart disease based on the indicators provided.**\n",
    "\n",
    "The model will include:\n",
    "\n",
    "0. Dataset\n",
    "1. train/test split\n",
    "2. Dataloader\n",
    "3. Batch\n",
    "4. GPU\n",
    "5. Training\n",
    "6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data\n",
    "This dataset is imported from [kaggle](https://kaggle.com). Kaggle is a wonderful website to collect datasets. Feel free to check it out!\n",
    "\n",
    "This dataset is from https://www.kaggle.com/datasets/alexteboul/heart-disease-health-indicators-dataset. It includes 253680 health indicators entries. Some indicators are BMI, age, diabetes, cholestrol, income, heart attack, etc. In this example, we are going to use heart attack as our label, using the rest data to predict whether a patient has heart attack or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253680, 15)\n",
      "   HeartDiseaseorAttack  HighBP  HighChol  CholCheck  Smoker  Stroke  \\\n",
      "0                     0       1         1          1       1       0   \n",
      "1                     0       0         0          0       1       0   \n",
      "2                     0       1         1          1       0       0   \n",
      "3                     0       1         0          1       0       0   \n",
      "4                     0       1         1          1       0       0   \n",
      "\n",
      "   Diabetes  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  AnyHealthcare  \\\n",
      "0         0             0       0        1                  0              1   \n",
      "1         0             1       0        0                  0              0   \n",
      "2         0             0       1        0                  0              1   \n",
      "3         0             1       1        1                  0              1   \n",
      "4         0             1       1        1                  0              1   \n",
      "\n",
      "   NoDocbcCost  DiffWalk  Sex  \n",
      "0            0         1    0  \n",
      "1            1         0    0  \n",
      "2            1         1    0  \n",
      "3            0         0    0  \n",
      "4            0         0    0  \n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv('Data/heart_disease_health_indicators_BRFSS2015.csv')\n",
    "print(dataframe.shape)\n",
    "print(dataframe.head()) # Print out the first 5 rows\n",
    "\n",
    "# First indicator is the label, so we skip it\n",
    "indicators = dataframe.iloc[:, 1:].values\n",
    "labels = dataframe.HeartDiseaseorAttack.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/valid/test split\n",
    "Spliting your data into different set is an important technique to evaluate your model. `sklearn` provides a function to achieve this. For more about train/valid/test split, check [this link](https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202944, 14)\n",
      "(50736, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(indicators, labels, test_size=0.2)\n",
    "\n",
    "# ===================\n",
    "# If you want to have train/valid/test split, you can split your test set again,\n",
    "# thus you have 3 sets of data: train, valid, and test.\n",
    "# ===================\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch/dataloader\n",
    "\n",
    "One thing that is elegant about matrix is that we can batch our training data in bulk. Doing matrix operation to train our model is way faster and efficient than sending data one by one. PyTorch provides us `DataLoader` to do this in a simpler manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Batched Input:\n",
      "tensor([[1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1],\n",
      "        [1, 0, 1, 1, 0, 2, 0, 1, 1, 0, 1, 0, 1, 1]])\n",
      "Batched Labels:\n",
      "tensor([0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = list(zip(X_train, Y_train))\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "\n",
    "dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "counter = 0\n",
    "batched_corpus, batched_labels = next(iter(dataloader))\n",
    "print(len(batched_corpus))\n",
    "print(\"Batched Input:\")\n",
    "print(batched_corpus)\n",
    "print(\"Batched Labels:\")\n",
    "print(batched_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Network\n",
    "Now we have equipped all the tools, let's put everything together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataframe = pd.read_csv('Data/heart_disease_health_indicators_BRFSS2015.csv')\n",
    "indicators = dataframe.iloc[:, 1:].values\n",
    "labels = dataframe.HeartDiseaseorAttack.values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(indicators, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hyper_params):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.hyper_params = hyper_params\n",
    "        \n",
    "        # model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(hyper_params['input_dim'], hyper_params['hidden_dim']),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hyper_params['hidden_dim'], hyper_params['output_dim']),\n",
    "            nn.LogSoftmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return torch.argmax(self.model(x), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "This is the most basic and simple framework of building neural network using PyTorch. \n",
    "- First, you need to specify your training hyperparameters. \n",
    "- Then, initialize your model, optimizer and loss functions. \n",
    "- Last, follow the workflow we have learned above and train your model iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 0/100 - Loss = 1368.977780342102\n",
      "Epoch 10/100 - Loss = 1366.5116505622864\n",
      "Epoch 20/100 - Loss = 1366.4105682373047\n",
      "Epoch 30/100 - Loss = 1366.409029006958\n",
      "Epoch 40/100 - Loss = 1366.3991341590881\n",
      "Epoch 50/100 - Loss = 1366.3535523414612\n",
      "Epoch 60/100 - Loss = 1366.3289322853088\n",
      "Epoch 70/100 - Loss = 1366.3732919692993\n",
      "Epoch 80/100 - Loss = 1366.3712034225464\n",
      "Epoch 90/100 - Loss = 1366.3232707977295\n"
     ]
    }
   ],
   "source": [
    "# This code will make our training on GPU if available\n",
    "print(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Hyperparameters\n",
    "hyper_params = {\n",
    "    \"batch_size\": 1024,\n",
    "    \"input_dim\" : len(X_train[0]),\n",
    "    \"hidden_dim\": int(len(X_train[0])),\n",
    "    \"output_dim\": 2,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"num_epochs\": 100,\n",
    "}\n",
    "\n",
    "# Batching data\n",
    "train_data = list(zip(X_train, Y_train))\n",
    "dataloader = DataLoader(train_data, batch_size=hyper_params['batch_size'], shuffle=True)\n",
    "\n",
    "# Initializing model/optimizer/loss function\n",
    "ffnn = FFNN(hyper_params)\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=hyper_params['learning_rate'])\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(hyper_params['num_epochs']):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        X_batch, Y_batch = batch\n",
    "        # Zero the grads!\n",
    "        optimizer.zero_grad()\n",
    "        pred = ffnn(X_batch.float())\n",
    "        loss = loss_function(pred, Y_batch)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Calculate total loss\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{hyper_params[\"num_epochs\"]} - Loss = {total_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & confusion matrix\n",
    "Now it is the time to test our test result. One way is to calculate [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for our model. \n",
    "\n",
    "Credits: https://www.cs.utexas.edu/~gdurrett/courses/fa2019/cs388.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35479 / 50736 = 0.699287;\n",
      "Precision (fraction of predicted positives that are correct): 3734 / 17911 = 0.208475;\n",
      "Recall (fraction of true positives predicted correctly): 3734 / 4814 = 0.775654;\n",
      "F1 (harmonic mean of precision and recall): 0.328625;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_evaluation(golds, predictions):\n",
    "    \"\"\"\n",
    "    Prints evaluation statistics comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
    "    Prints accuracy as well as precision/recall/F1 of the positive class, which can sometimes be informative if either\n",
    "    the golds or predictions are highly biased.\n",
    "\n",
    "    :param golds: gold labels\n",
    "    :param predictions: pred labels\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_pos_correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    num_total = 0\n",
    "    if len(golds) != len(predictions):\n",
    "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" % (len(golds), len(predictions)))\n",
    "    for idx in range(0, len(golds)):\n",
    "        gold = golds[idx]\n",
    "        prediction = predictions[idx]\n",
    "        if prediction == gold:\n",
    "            num_correct += 1\n",
    "        if prediction == 1:\n",
    "            num_pred += 1\n",
    "        if gold == 1:\n",
    "            num_gold += 1\n",
    "        if prediction == 1 and gold == 1:\n",
    "            num_pos_correct += 1\n",
    "        num_total += 1\n",
    "    acc = float(num_correct) / num_total\n",
    "    output_str = \"Accuracy: %i / %i = %f\" % (num_correct, num_total, acc)\n",
    "    prec = float(num_pos_correct) / num_pred if num_pred > 0 else 0.0\n",
    "    rec = float(num_pos_correct) / num_gold if num_gold > 0 else 0.0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if prec > 0 and rec > 0 else 0.0\n",
    "    output_str += \";\\nPrecision (fraction of predicted positives that are correct): %i / %i = %f\" % (num_pos_correct, num_pred, prec)\n",
    "    output_str += \";\\nRecall (fraction of true positives predicted correctly): %i / %i = %f\" % (num_pos_correct, num_gold, rec)\n",
    "    output_str += \";\\nF1 (harmonic mean of precision and recall): %f;\\n\" % f1\n",
    "    print(output_str)\n",
    "\n",
    "pred_test = ffnn.predict(torch.Tensor(X_test))\n",
    "print_evaluation(Y_test, pred_test.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is wrong?\n",
    "Though we achieve a good accuracy, but if you paid close attention to precision score, it is not ideal. Our model having a bad performance when predicting positives. With that being said, even though our model achieves relatively high accuracy, but it may tend to predict non-positives more. Why does this happen?\n",
    "\n",
    "Let's check our training data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive label: 9.401115578681804%\n"
     ]
    }
   ],
   "source": [
    "print(f'Percentage of positive label: {sum(Y_train)/len(Y_train) * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This dataset is super imbalanced! Our data only has 9% positive labels. Thus, our model is trained with a huge ammount of \"0\" labels. No wonder it tends to predict \"0\" more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is next?\n",
    "Here I demonstrate a simple toy task to show you how to build a neural network from scratch. As you can see, there is a lot more we can improve. Some points we can work on:\n",
    "- Collect a better (balanced) dataset\n",
    "- Preprocessing your dataset\n",
    "- Try different combinations of layers (adding dropouts, initializing layers differently)\n",
    "- Play with different combinations of optimizers(`SGD`) and loss functions(`BCELoss`)\n",
    "- Explore different tasks (NLP: sentiment analysis, CV: multiclass classification)\n",
    "- Try out different models (LSTM, RNN, transformers, CNN)\n",
    "- Research on different evaluation method\n",
    "\n",
    "Thanks for reading! Wish you all the best and have a wonderful life <3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75273f6ed8af91899ebc591bf6ae2fd0716c5db2515d7097a000123632f4e53a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
